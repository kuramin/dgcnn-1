{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kuramin/Diploma/repos/DGCNN_tf/dgcnn-1/tensorflow\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(os.path.join(BASE_DIR, 'utils'))\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import provider\n",
    "from utils import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "NUM_POINT = 1024\n",
    "MAX_EPOCH = 50 #250\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = importlib.import_module('dgcnn')  # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'models', 'dgcnn.py')\n",
    "LOG_DIR = 'log'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "# os.system('cp %s %s' % (MODEL_FILE, LOG_DIR)) # bkp of model def\n",
    "# os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "LOG_FOUT.write(\"BATCH_SIZE = 24, NUM_POINT = 1024, MAX_EPOCH = 250, BASE_LEARNING_RATE = 0.001, GPU_INDEX = 0, MOMENTUM = 0.9, OPTIMIZER = 'adam', DECAY_STEP = 200000, DECAY_RATE = 0.7\\n\")\n",
    "\n",
    "MAX_NUM_POINT = 4096 #2048\n",
    "NUM_CLASSES = 40\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelNet40 official train/test split\n",
    "TRAIN_FILES = provider.get_data_files( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/train_files.txt'))\n",
    "TEST_FILES = provider.get_data_files( \\\n",
    "    os.path.join(BASE_DIR, 'data/modelnet40_ply_hdf5_2048/test_files.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(log_file, out_str):\n",
    "    log_file.write(out_str + '\\n')\n",
    "    log_file.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        DECAY_STEP,  # Decay step.\n",
    "        DECAY_RATE,  # Decay rate.\n",
    "        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001)  # don't allow learning rate go beyond 0.00001\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "        BN_INIT_DECAY,\n",
    "        batch * BATCH_SIZE,\n",
    "        BN_DECAY_DECAY_STEP,\n",
    "        BN_DECAY_DECAY_RATE,\n",
    "        staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "\n",
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:' + str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE,\n",
    "                                                                 NUM_POINT)  # create placeholders using method\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get prediction and loss based on placeholders\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl,\n",
    "                                               bn_decay=bn_decay)  # predictions on probabilities of BATCH_SIZE clouds belonging to 40 classes\n",
    "            loss = MODEL.get_loss(pred, labels_pl,\n",
    "                                  end_points)  # value of loss for batch, based on difference of predictions and true values of classes for BATCH_SIZE clouds\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(\n",
    "                labels_pl))  # vector of booleans of correct guesses, based on predictions and and true values of classes for BATCH_SIZE clouds\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(\n",
    "                BATCH_SIZE)  # cast vector of corrects to float, define number of corrects and divide by total number\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            # Get training operator\n",
    "            learning_rate = get_learning_rate(batch)  # calculate current value of learning rate\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)  # define what does optimizer minimize\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Add summary writers\n",
    "        # merged = tf.merge_all_summaries()\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'train'),\n",
    "                                             sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        # Init variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        # sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'labels_pl': labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "        # for every epoch in range 0 to MAX_EPOCH - 1 do train, evaluation and saving\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "            log_string(LOG_FOUT, '**** EPOCH %03d ****' % (epoch))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # training within every epoch will be done using 5 train-files,\n",
    "            # dividing each into pieces of size BATCH_SIZE\n",
    "            # BATCH_SIZE of clouds with different labels\n",
    "            train_one_epoch(sess, ops, train_writer)\n",
    "\n",
    "            # evaluation within every epoch\n",
    "            eval_one_epoch(sess, ops, test_writer)\n",
    "\n",
    "            # Save the variables to disk.\n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"))\n",
    "                log_string(LOG_FOUT, \"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "# define function which creates data batches and feeds them to a built graph for training\n",
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "\n",
    "    # Shuffle train files\n",
    "    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    np.random.shuffle(train_file_idxs)  # train_file_idxs will be some random sequence of indexes of train files\n",
    "    print(\"train_file_idxs is\", train_file_idxs)\n",
    "\n",
    "    # training within every epoch will be done using 5 train-files\n",
    "    for fn in range(len(TRAIN_FILES)):\n",
    "        log_string(LOG_FOUT, 'train file ----' + str(fn) + '-----')\n",
    "        # train_file_idxs[fn] is some random index of train file.\n",
    "        # current_data will be 2048 clouds with 2048 (MAX_NUM_POINT) points in each, 3 coors per point\n",
    "        # current_label will be 2048 clouds with 1 label each\n",
    "        # current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data, current_label = provider.load_h5(TRAIN_FILES[train_file_idxs[fn]])\n",
    "        current_data = current_data[:, 0:NUM_POINT, :]  # cut only specified number of points from each cloud\n",
    "        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))\n",
    "        # current_label = np.squeeze(current_label)\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "\n",
    "        total_correct = 0\n",
    "        total_seen = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        # clouds within every file will be divided into groups of size BATCH_SIZE\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            # Cut a sequence of BATCH_SIZE clouds from current_data\n",
    "            # And augment batched point clouds by rotation and jittering\n",
    "            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "            jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "            jittered_data = provider.random_scale_point_cloud(jittered_data)\n",
    "            jittered_data = provider.rotate_perturbation_point_cloud(jittered_data)\n",
    "            jittered_data = provider.shift_point_cloud(jittered_data)\n",
    "\n",
    "            # define dict and perform feeding of this batch\n",
    "            feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training, }\n",
    "            summary, step, _, loss_value, pred_value = sess.run([ops['merged'], ops['step'],\n",
    "                                                                 ops['train_op'], ops['loss'], ops['pred']],\n",
    "                                                                feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            pred_value = np.argmax(pred_value, 1)  # pred_value is argmax of pred_value probabilities on axis 1\n",
    "            correct = np.sum(pred_value == current_label[start_idx:end_idx])  # number of correct predictions\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += loss_value\n",
    "\n",
    "        log_string(LOG_FOUT, 'mean loss: %f' % (loss_sum / float(num_batches)))  # mean loss of current epoch\n",
    "        log_string(LOG_FOUT, 'accuracy: %f' % (\n",
    "                    total_correct / float(total_seen)))  # accuracy of current epoch. Both numbers count clouds\n",
    "\n",
    "\n",
    "# define function which creates data batches and feeds them to a built graph for testing\n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "\n",
    "    # evaluation within every epoch will be done using 5 train-files\n",
    "    for fn in range(len(TEST_FILES)):\n",
    "        log_string(LOG_FOUT, 'eval file ----' + str(fn) + '-----')\n",
    "        #current_data, current_label = provider.loadDataFile(TEST_FILES[fn])  # load data and labels from file\n",
    "        current_data, current_label = provider.load_h5(TEST_FILES[fn])  # load data and labels from file\n",
    "        current_data = current_data[:, 0:NUM_POINT, :]  # crop only specific number of points from current_data\n",
    "        current_label = np.squeeze(current_label)  # no need to shuffle, so that's it\n",
    "\n",
    "        file_size = current_data.shape[0]\n",
    "        num_batches = file_size // BATCH_SIZE\n",
    "\n",
    "        # clouds within every file will be divided into groups of size BATCH_SIZE\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * BATCH_SIZE\n",
    "            end_idx = (batch_idx + 1) * BATCH_SIZE\n",
    "\n",
    "            # define dict and perform feeding of this batch\n",
    "            feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                         ops['labels_pl']: current_label[start_idx:end_idx],\n",
    "                         ops['is_training_pl']: is_training}\n",
    "            summary, step, loss_value, pred_value = sess.run([ops['merged'], ops['step'],\n",
    "                                                              ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "            pred_value = np.argmax(pred_value, 1)  # pred_value is argmax of pred_value probabilities on axis 1\n",
    "            correct = np.sum(pred_value == current_label[start_idx:end_idx])  # number of correct predictions\n",
    "            total_correct += correct\n",
    "            total_seen += BATCH_SIZE\n",
    "            loss_sum += (loss_value * BATCH_SIZE)\n",
    "            #loss_sum += loss_value\n",
    "            for i in range(start_idx, end_idx):\n",
    "                l = current_label[i]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_value[i - start_idx] == l)\n",
    "\n",
    "    log_string(LOG_FOUT, 'eval mean loss: %f' % (loss_sum / float(total_seen)))\n",
    "    #log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string(LOG_FOUT, 'eval accuracy: %f' % (total_correct / float(total_seen)))\n",
    "    log_string(LOG_FOUT, 'eval avg class acc: %f' % (\n",
    "        np.mean(np.array(total_correct_class) / np.array(total_seen_class, dtype=np.float))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "**** EPOCH 000 ****\n",
      "train_file_idxs is [0 2 3 4 1]\n",
      "train file ----0-----\n",
      "mean loss: 3.571522\n",
      "accuracy: 0.183333\n",
      "train file ----1-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-26cea9f82645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mLOG_FOUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-12ebfc47f9cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# dividing each into pieces of size BATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# BATCH_SIZE of clouds with different labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# evaluation within every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-12ebfc47f9cb>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(sess, ops, train_writer)\u001b[0m\n\u001b[1;32m    163\u001b[0m             summary, step, _, loss_value, pred_value = sess.run([ops['merged'], ops['step'],\n\u001b[1;32m    164\u001b[0m                                                                  ops['train_op'], ops['loss'], ops['pred']],\n\u001b[0;32m--> 165\u001b[0;31m                                                                 feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pred_value is argmax of pred_value probabilities on axis 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_gpu_tf1.11/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    LOG_FOUT.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
